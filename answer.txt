Performance Analysis: File Transfer Execution Times vs File Size

Table(Check graph in the image):
File Size (MB) | Execution Time (s) | Throughput (MB/s)
--------------------------------------------------
         1     |       0.663        |      1.51
         5     |       1.453        |      3.44
        10     |       2.572        |      3.89
        25     |       5.076        |      4.93
        50     |       9.058        |      5.52
       100     |      16.696        |      5.99

Average Throughput: 4.21 MB/s

Analisis:
The graph(check pdf or image) shows a clear quadratic relationship between file size and execution time. As file size increases, execution time grows at an accelerating rate rather than linearly. This is evident from the data:

- 1MB → 5MB (5x size increase): 2.2x time increase
- 5MB → 25MB (5x size increase): 3.5x time increase  
- 25MB → 100MB (4x size increase): 3.3x time increase

The throughput improves slightly with larger files (from 1.51 MB/s to 5.99 MB/s) because the fixed overhead per operation becomes proportionally smaller for larger files.


bottlenecks for diff types of file sizes :

Small Files - Fixed communication Overhead
Medium Files - Mix of overhead and transfer time
Large Files - Primarily data transfer time

Reason for that performance trend:

1. Buffer Size (Primary Factor):
The system uses a default 256-byte buffer size, which creates the main performance bottleneck. For a 100MB file, this requires approximately 390,625 separate read/write operations. Each operation involves:
- System call overhead for named pipe communication
- Process context switching between client and server
- Buffer allocation and deallocation

2. Name pipe communication overhead:
Each chunk transfer requires multiple system calls through named pipes (FIFOs). The inter-process communication adds significant latency compared to in-memory operations. The overhead is constant per operation but becomes more significant with smaller chunk sizes.

3. sequential proccessing :
The client processes file chunks sequentially, waiting for each response before requesting the next chunk. This prevents any parallel processing and adds cumulative latency.

4. Memory management overhead:
Each chunk transfer involves dynamic memory allocation and deallocation, which adds computational overhead that scales with the number of operations.

5. quadratic growth pattern:
The slightly quadratic growth (rather than linear) occurs because:
- System resource contention increases with longer transfer times
- Memory fragmentation may worsen during long transfers
- Named pipe buffer management becomes less efficient with sustained high throughput

